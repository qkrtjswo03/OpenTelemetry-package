# **0. ì‚¬ì „ ì¤€ë¹„**

- ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±

```
kubectl create ns monitoring      # ê·¸ë¼íŒŒë‚˜, í”„ë¡œë©”í…Œìš°ìŠ¤, ë¡œí‚¤ê°€ ë°°í¬ë  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
kubectl create ns minio           # Loki, Tempo ë°ì´í„°ê°€ ì €ì¥ë  MinIO ê°€ ë°°í¬ë  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
kubectl create ns tempo           # Tempo-distrubited ì»´í¬ë„ŒíŠ¸ë“¤ì´ ë°°í¬ ë  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
kubectl create ns otel-trace      # OpenTelemetry Collector ê°€ ë°°í¬ ë  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
kubectl create ns test            # ìƒ˜í”Œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì´ ì˜¬ë¼ê°ˆ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
```

---

- `2. biz` ë””ë ‰í† ë¦¬ êµ¬ì„±ì€ ì•„ë˜ì™€ ê°™ìœ¼ë©° ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì„¤ì¹˜ë¥¼ ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

```
cd 1.biz ; ll
drwxr-xr-x 4 root root 4096 May 21 14:30 1.minio/
drwxr-xr-x 4 root root 4096 May 21 09:45 2.kube-prometheus-stack/
drwxr-xr-x 4 root root 4096 May 20 12:35 3.promtail/
drwxr-xr-x 4 root root 4096 May 20 16:55 4.loki-distriuted/
drwxr-xr-x 5 root root 4096 May 21 14:27 5.tempo-distributed/
drwxr-xr-x 3 root root 4096 May 21 14:28 6.OpenTelemetry/
```

---

# **1. MinIO Install**

## **1-1. Minio secret ìƒì„± ( Biz â†’ Mgmt Minio ìš©ë„ )**

- ğŸ” `minio-key.yaml` íŒŒì¼ ë‚´ìš© ì„¤ëª…

```
cat minio-key.yaml
type: s3
config:
  bucket: thanos
  endpoint: 121.141.64.224:32000# Bastion Public IP : NodePort
  access_key: minioadmin
  secret_key: minioadmin
  insecure:true
  signature_version2:true
```

- âœ… ì´ ì„¤ì •ì€ **Biz** ì˜ object storage backendë¡œ **Mgmt** ì˜ **MinIO**ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤.

```
kubectl create secret generic thanos-minio-secret -n monitoring --from-file=minio-key.yaml
```

- ìƒì„± í™•ì¸

```
kubectl get secrets | grep thanos
thanos-minio-secret       Opaque         1      47d      # secret generic ìƒì„±
```

## **1-2. Minio Pod ë°°í¬ [ Biz ìš© Object Storage ( Loki,Tempo ) ]**

- persistence: storageClass: â€œìˆ˜ì •â€
- node label ì§€ì • : `kubectl label nodes [NodeName] key=value`

```
persistence:
  ## @param persistence.enabled Enable MinIO&reg; data persistence using PVC. If false, use emptyDir
  enabled: true
  storageClass: "ìˆ˜ì •"
provisioning:
  nodeSelector:
    minio: "true"
```

```
~/backup/viola-monitoring/1.mgmt/1.minio# helm install minio . -n minio
```

- ë°°í¬ í™•ì¸

```
kubectl get all -n minio
NAME                         READY   STATUS    RESTARTS   AGE
pod/minio-856d97c98c-lm7zd   1/1     Running   0          47h
NAME            TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)                         AGE
service/minio   NodePort   10.10.0.97   <none>        9000:31000/TCP,9001:31002/TCP   47h
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/minio   1/1     1            1           47h
NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/minio-856d97c98c   1         1         1       47h
```

## **1-3. Biz HAProxy ì„¤ì • ( MinIO )**

```
# MGMT MINIO(METRCICS) Web Console ######################################
listen minio-api
  bind *:32000
  mode tcp
  server k8s-master01 182.168.10.11:32000  check inter 2000 rise 2 fall 5
  server k8s-master02 182.168.10.12:32000  check inter 2000 rise 2 fall 5
  server k8s-master03 182.168.10.13:32000  check inter 2000 rise 2 fall 5
  server k8s-worker01 182.168.10.21:32000  check inter 2000 rise 2 fall 5
  server k8s-worker02 182.168.10.22:32000  check inter 2000 rise 2 fall 5
  server k8s-worker03 182.168.10.23:32000  check inter 2000 rise 2 fall 5
listen minio-console
  bind *:32001
  mode tcp
  server k8s-master01 182.168.10.11:32001  check inter 2000 rise 2 fall 5
  server k8s-master02 182.168.10.12:32001  check inter 2000 rise 2 fall 5
  server k8s-master03 182.168.10.13:32001  check inter 2000 rise 2 fall 5
  server k8s-worker01 182.168.10.21:32001  check inter 2000 rise 2 fall 5
  server k8s-worker02 182.168.10.22:32001  check inter 2000 rise 2 fall 5
  server k8s-worker03 182.168.10.23:32001  check inter 2000 rise 2 fall 5
#########################################################################
```

- ì½˜ì†” ì ‘ì† í™•ì¸
    - [[ BIZ ] MINIO_Console](http://121.141.64.224:31002/browser) : `Default Bucket : k8s-Loki, k8s-Tempo`

Open image-20250521-075631.png

---

# **2. kube-prometheus-stack Install**

- `kube-prometheus-stack : kube-prometheus-stack-66.4.0`

## **2-1. Values.yaml ìˆ˜ì •**

- persistence: storageClass: â€œìˆ˜ì •â€
- node label ì§€ì • : `kubectl label nodes [NodeName] monitoring=true`
- kubeEtcd: endpoints: `â€œmasterIP ìˆ˜ì •â€`

```
grafana:
  persistence:
    enabled: true
    type: sts
    storageClassName: "ìˆ˜ì •"
    accessModes:
      - ReadWriteOnce
    size: 20Gi
    finalizers:
      - kubernetes.io/pvc-protection
prometheusSpec:
    nodeSelector:
      monitoring: "true"
kubeEtcd:
  enabled: true
  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  endpoints:
   - 10.10.0.101
   - 10.10.0.102
   - 10.10.0.103
```

## **2-2. Prometheus-stack helm install**

```
~/backup/viola-monitoring/1.mgmt/2.kube-prometheus-stack# helm install prometheus . -n monitoring
```

- ë°°í¬ í™•ì¸

```
kubectl get pods -n monitoring
NAME                                                     READY   STATUS    RESTARTS   AGE
alertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          7d22h
prometheus-grafana-0                                     3/3     Running   0          28h
prometheus-kube-prometheus-operator-67547cc956-gm86t     1/1     Running   0          7d22h
prometheus-kube-state-metrics-66f5694654-mswhn           1/1     Running   0          7d22h
prometheus-prometheus-kube-prometheus-prometheus-0       3/3     Running   0          7d22h
prometheus-prometheus-kube-prometheus-prometheus-1       3/3     Running   0          7d22h
prometheus-prometheus-node-exporter-9ns6f                1/1     Running   0          7d22h
prometheus-prometheus-node-exporter-hc7tv                1/1     Running   0          7d22h
prometheus-prometheus-node-exporter-kp6jq                1/1     Running   0          7d22h
prometheus-prometheus-node-exporter-sf6t5                1/1     Running   0          7d22h
prometheus-prometheus-node-exporter-vp8rg                1/1     Running   0          7d22h
prometheus-prometheus-node-exporter-wjjqq                1/1     Running   0          7d22h
kubectl get svc -n monitoring | grep NodePort
prometheus-grafana                            NodePort    10.10.0.174   <none>        80:32222/TCP                      7d22h
prometheus-kube-prometheus-prometheus         NodePort    10.10.0.128   <none>        9090:32221/TCP,8080:31531/TCP     7d22h
prometheus-kube-prometheus-thanos-discovery   NodePort    10.10.0.36    <none>        10901:30901/TCP,10902:30902/TCP   7d22h
thanos-query-frontend                         NodePort    10.10.0.176   <none>        9090:32223/TCP                    2d4h
```

## **2-3. Biz HAProxy ì„¤ì • ( Prometheus, Grafana )**

- `Grafana NodePort` : `32224`
- `Prometheus NodePort` : `30090`

```
# BIZ Prometheus,Grafana Console ########################################
listen biz-Grafana
  bind *:32224
  mode http
  server biz-master01 182.168.10.111:32224  check inter 2000 rise 2 fall 5
  server biz-master02 182.168.10.112:32224  check inter 2000 rise 2 fall 5
  server biz-master03 182.168.10.113:32224  check inter 2000 rise 2 fall 5
  server biz-worker01 182.168.10.121:32224  check inter 2000 rise 2 fall 5
  server biz-worker02 182.168.10.122:32224  check inter 2000 rise 2 fall 5
  server biz-worker03 182.168.10.123:32224  check inter 2000 rise 2 fall 5
listen biz-prometheus
  bind *:30090
  mode http
  server biz-master01 182.168.10.111:30090  check inter 2000 rise 2 fall 5
  server biz-master02 182.168.10.112:30090  check inter 2000 rise 2 fall 5
  server biz-master03 182.168.10.113:30090  check inter 2000 rise 2 fall 5
  server biz-worker01 182.168.10.121:30090  check inter 2000 rise 2 fall 5
  server biz-worker02 182.168.10.122:30090  check inter 2000 rise 2 fall 5
  server biz-worker03 182.168.10.123:30090  check inter 2000 rise 2 fall 5

```

---

# **3. Promtail Install**

- `Chart Version : promtail-6.16.6`
- Promtail Helm ë°°í¬

```
:~/backup/viola-monitoring/2.biz/3.promtail# helm install promtail . -n monitoring
```

---

# **4. Loki-distributed Install**

- `Chart Version : loki-distributed-0.80.2`

## **4-1. Values.yaml ìˆ˜ì •**

- persistence: storageClass: â€œìˆ˜ì •â€
- node label ì§€ì • : `kubectl label nodes [NodeName] monitoring=true`

```
ingester:
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: true
        storageClass: "ìˆ˜ì •"
indexGateway:
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: true
        storageClass: "ìˆ˜ì •"
distributor:
  nodeSelector:
    monitoring: "true"
querier:
  nodeSelector:
    monitoring: "true"
query-frontend:
  nodeSelector:
    monitoring: "true"
```

- Loki Distributed Helm ë°°í¬

```
:~/backup/viola-monitoring/2.biz/4.loki-distriuted# helm install loki . -n monitoring
```

- ì•„ë˜ì™€ ê°™ì´ `loki-loki-distributed-gateway ì„œë¹„ìŠ¤ê°€ ë…¸ë“œí¬íŠ¸ë¡œ ì„¤ì •ì´ ë˜ì–´ ìˆì–´ì•¼ ë©ë‹ˆë‹¤. (MGMT Grafana ì—ì„œ BIZ Loki Datasource ì—°ê²°í•˜ê¸° ìœ„í•¨)`

```
loki-loki-distributed-gateway                   NodePort    10.10.0.75    <none>        80:31231/TCP                      47h
```

## **4-2. Biz HAProxy ì„¤ì • (Bastion)**

```
# BIZ Loki-distributed-querier ###########################################
listen loki-loki-distributed-querier
  bind *:31231
  mode tcp
  server biz-master01 182.168.10.111:31231  check inter 2000 rise 2 fall 5
  server biz-master02 182.168.10.112:31231  check inter 2000 rise 2 fall 5
  server biz-master03 182.168.10.113:31231  check inter 2000 rise 2 fall 5
  server biz-worker01 182.168.10.121:31231  check inter 2000 rise 2 fall 5
  server biz-worker02 182.168.10.122:31231  check inter 2000 rise 2 fall 5
  server biz-worker03 182.168.10.123:31231  check inter 2000 rise 2 fall 5
```

## **4-3. Grafana DataSource ì—°ê²°**

- [BIZ] Grafana Loki DataSource : `http://loki-loki-distributed-gateway:80`
- [MGMT] Grafana Loki DataSource : `http://121.141.64.224:31231`

---

# **5. Tempo-distributed Install**

- `Chart Version : tempo-distributed-1.32.2`

## **5-1. Values.yaml ìˆ˜ì •**

- persistence: storageClass: â€œìˆ˜ì •â€
- node label ì§€ì • : `kubectl label nodes [NodeName] monitoring=true`

```
metricsGenerator:
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: true
    storageClass: "ìˆ˜ì •"
ingester:
  persistence:
  # -- Enable creating PVCs which is required when using boltdb-shipper
  enabled: true
  storageClass: "ìˆ˜ì •"
```

- Tempo Distributed Helm ë°°í¬

```
:~/backup/viola-monitoring/2.biz/5.Tempo-distriuted# helm install tempo . -n tempo
```

- ì•„ë˜ì™€ ê°™ì´ `tempo-gateway ì„œë¹„ìŠ¤ê°€ ë…¸ë“œí¬íŠ¸ë¡œ ì„¤ì •ì´ ë˜ì–´ ìˆì–´ì•¼ ë©ë‹ˆë‹¤. (MGMT Grafana ì—ì„œ BIZ Tempo Datasource ì—°ê²°í•˜ê¸° ìœ„í•¨)`

```
tempo-gateway                       NodePort    10.10.0.120   <none>        80:31538/TCP                                    9m48s
```

## **5-2. Biz HAProxy ì„¤ì • (Bastion)**

```
# BIZ tempo-distributed ##########
listen tempo-query-frontend
  bind *:31538
  mode tcp
  server biz-master01 182.168.10.111:31538  check inter 2000 rise 2 fall 5
  server biz-master02 182.168.10.112:31538  check inter 2000 rise 2 fall 5
  server biz-master03 182.168.10.113:31538  check inter 2000 rise 2 fall 5
  server biz-worker01 182.168.10.121:31538  check inter 2000 rise 2 fall 5
  server biz-worker02 182.168.10.122:31538  check inter 2000 rise 2 fall 5
  server biz-worker03 182.168.10.123:31538  check inter 2000 rise 2 fall 5
```

## **5-3. Grafana DataSource ì—°ê²°**

- [BIZ] Grafana Tempo DataSource : `http://tempo-query-frontend-discovery.tempo:3100`
- [MGMT] Grafana Tempo DataSource : `http://121.141.64.224:31538`

---

# **6. OpenTelemetry Install**

- ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë°°í¬ í•©ë‹ˆë‹¤.

```
-rw-r--r-- 1 root root 986843 Oct  9  2024 1.cert-manager.yaml
-rw-r--r-- 1 root root 701222 May  9 15:26 2.opentelemetry-operator.yaml
-rw-r--r-- 1 root root   4473 May 21 12:37 3.OpenTelemetryCollector.yaml
-rw-r--r-- 1 root root    602 May  9 15:26 4.otel-collector-spanmetrics.yaml
drwxr-xr-x 2 root root   4096 May 23 09:45 5.instrumentations/
drwxr-xr-x 3 root root   4096 May 21 13:45 sample-application/
```

## **6-1. Cert-Manager Install**

- cert-managerëŠ” Let's Encrypt ê°™ì€ ì¸ì¦ ê¸°ê´€(CA)ê³¼ í†µì‹ í•˜ì—¬ **TLS ì¸ì¦ì„œë¥¼ ìë™ ë°œê¸‰í•˜ê³  ê°±ì‹ **í•´ì£¼ëŠ” Kubernetesìš© ì»¨íŠ¸ë¡¤ëŸ¬ì…ë‹ˆë‹¤.

```
kubectl apply -f 1.cert-manager.yaml
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.1/cert-manager.yaml
```

```
kubectl get pods -n cert-manager
NAME                                      READY   STATUS    RESTARTS       AGE
cert-manager-cainjector-fb79858b4-5ssb4   1/1     Running   18 (37h ago)   2d16h
cert-manager-fbbb9fdd5-tsmcb              1/1     Running   12 (37h ago)   2d16h
cert-manager-webhook-6cc5985dd5-kjx6h     1/1     Running   1 (18h ago)    2d16h
```

## **6-2. OpenTelemetry Operator Install**

- **OpenTelemetry Collector**, **Auto-instrumentation**, **CustomResource ê´€ë¦¬**ë¥¼ ìë™í™”í•´ì£¼ëŠ” Kubernetes ì˜¤í¼ë ˆì´í„°ì…ë‹ˆë‹¤.

```
kubectl apply -f 2.opentelemetry-operator.yaml
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
```

```
kubectl get pods -n opentelemetry-operator-system
NAME                                                         READY   STATUS    RESTARTS       AGE
opentelemetry-operator-controller-manager-867cbbd5d4-gl6l4   2/2     Running   15 (37h ago)   2d17h
```

## **6-3. OpenTelemetry Collector Install**

- **OpenTelemetry Collector**ëŠ” ë‹¤ì–‘í•œ í˜•ì‹ì˜ telemetry ë°ì´í„°(ë¡œê·¸, ë©”íŠ¸ë¦­, íŠ¸ë ˆì´ìŠ¤)ë¥¼ ìˆ˜ì§‘Â·ë³€í™˜Â·ì „ì†¡í•˜ëŠ” ë²¤ë” ì¤‘ë¦½ì ì¸ íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.

```
~/backup/viola-monitoring/2.biz/6.OpenTelemetry# kubectl apply -f 3.OpenTelemetryCollector.yaml
```

```
~/backup/viola-monitoring/2.biz/6.OpenTelemetry# kubectl get pods -n otel-trace
NAME                                   READY   STATUS    RESTARTS      AGE
collector-collector-668d99875b-x6phg   1/1     Running   1 (18h ago)   45h
```

## **6-4. OpenTelemetry SpanMetrics Install**

- **Prometheus Operatorìš©** `ServiceMonitor` ë¦¬ì†ŒìŠ¤ë¡œ, OpenTelemetry Collectorì˜ **SpanMetrics** ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤.

```
kubectl apply -f 4.otel-collector-spanmetrics.yaml
```

```
kubectl get servicemonitors.monitoring.coreos.com  -n otel-trace
NAME                         AGE
otel-collector-spanmetrics   2d17h
```

## **6-5. Auto-Instrument**

- ê°œë°œ ì–¸ì–´ì— ë§ê²Œ ì„ íƒì ìœ¼ë¡œ ë°°í¬ í•©ë‹ˆë‹¤.

```
-rw-r--r-- 1 root root  661 May  9 15:26 java-instrument.yaml
-rw-r--r-- 1 root root  372 May 21 14:20 python-instrument.yaml
```

```
kubectl get instrumentations.opentelemetry.io -n test
NAME                     AGE   ENDPOINT                                                       SAMPLER   SAMPLER ARG
python-instrumentation   44h   http://collector-collector.otel-trace.svc.cluster.local:4318
```

- Auto-Instrumentë¥¼ ì ìš©í•  í…ŒìŠ¤íŠ¸ íŒŒë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```
kubectl get pods -n test
viola-multicluster-app-7c8678b5c7-hctt6   1/1     Running   0          15h
```

- spec: template: metadata: annotations: ì•„ë˜ ë‘ì¤„ ì‚½ì…

```
        instrumentation.opentelemetry.io/inject-python: "true"
        instrumentation.opentelemetry.io/python-container-names: api
```

- ì ìš© í™•ì¸

```
kubectl describe pods viola-multicluster-app-7c8678b5c7-hctt6
Name:             viola-multicluster-app-7c8678b5c7-hctt6
Namespace:        test
...
Annotations:      instrumentation.opentelemetry.io/inject-python: true
                  instrumentation.opentelemetry.io/python-container-names: api
Status:           Running
IP:               100.100.3.3
IPs:
  IP:           100.100.3.3
Controlled By:  ReplicaSet/viola-multicluster-app-7c8678b5c7
...
Init Containers:
  opentelemetry-auto-instrumentation-python:
    Container ID:  containerd://08fe1c92485e04f7b31ce1a176af7f56874978688037911a8674dace25fb7828
    Image:         ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:0.53b1
    Image ID:      ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python@sha256:1fe31f53171ffaa5eb8267ba4ba481197bb186821caec2d711a4697bf05550b0
    Port:          <none>
    Host Port:     <none>
...
Containers:
  api:
    Container ID:   containerd://d788fa9b9d16c0a8c448f771ab35c9f005daa53343caeee49b763ea7211d6734
    Image:          lbg3977/viola-multicluster-app
    Image ID:       docker.io/lbg3977/viola-multicluster-app@sha256:5319211c1189e42625de52942700a7fbd9a0cc4870b00ce2064ef6a17732e2b9
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 22 May 2025 19:02:17 +0900
    Ready:          True
    Restart Count:  0
    Environment:
      OTEL_NODE_IP:                                       (v1:status.hostIP)
      OTEL_POD_IP:                                        (v1:status.podIP)
      PYTHONUNBUFFERED:                                  1
      API_HOST:                                          0.0.0.0
      API_PORT:                                          3000
      OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED:  true
      PYTHONPATH:                                        /otel-auto-instrumentation-python/opentelemetry/instrumentation/auto_instrumentation:/otel-auto-instrumentation-python
      OTEL_EXPORTER_OTLP_PROTOCOL:                       http/protobuf
      OTEL_TRACES_EXPORTER:                              otlp
      OTEL_METRICS_EXPORTER:                             otlp
      OTEL_LOGS_EXPORTER:                                otlp
      OTEL_SERVICE_NAME:                                 viola-multicluster-app
      OTEL_EXPORTER_OTLP_ENDPOINT:                       http://collector-collector.otel-trace.svc.cluster.local:4318
      OTEL_RESOURCE_ATTRIBUTES_POD_NAME:                 viola-multicluster-app-7c8678b5c7-hctt6 (v1:metadata.name)
      OTEL_RESOURCE_ATTRIBUTES_NODE_NAME:                 (v1:spec.nodeName)
      OTEL_PROPAGATORS:                                  tracecontext,baggage
      OTEL_RESOURCE_ATTRIBUTES:                          k8s.container.name=api,k8s.deployment.name=viola-multicluster-app,k8s.namespace.name=test,k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.replicaset.name=viola-multicluster-app-7c8678b5c7,service.instance.id=test.$(OTEL_RESOURCE_ATTRIBUTES_POD_NAME).api,service.namespace=test
    Mounts:
      /app/logs from logs (rw)
      /home/appuser/.kube from kubeconfig (ro)
      /otel-auto-instrumentation-python from opentelemetry-auto-instrumentation-python (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lrqps (ro)
```

- í•´ë‹¹ íŒŒë“œ ë‚´ì— `otel-auto-instrumentation-python` Agent íŒŒì¼ í™•ì¸

```
kubectl exec -it viola-multicluster-app-7c8678b5c7-hctt6 -- sh
Defaulted container "api" out of: api, opentelemetry-auto-instrumentation-python (init)
appuser@viola-multicluster-app-7c8678b5c7-hctt6:/$ ls
app  bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  otel-auto-instrumentation-python  proc  root  run  sbin  srv  sys  tmp  usr  var
```

---
